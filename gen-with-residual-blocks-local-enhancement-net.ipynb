{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9649262,"sourceType":"datasetVersion","datasetId":5893483},{"sourceId":9762959,"sourceType":"datasetVersion","datasetId":5978788},{"sourceId":9785519,"sourceType":"datasetVersion","datasetId":5995513}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nimport os\nimport time\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom IPython import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-26T09:26:38.182820Z","iopub.execute_input":"2024-11-26T09:26:38.183148Z","iopub.status.idle":"2024-11-26T09:26:38.188027Z","shell.execute_reply.started":"2024-11-26T09:26:38.183119Z","shell.execute_reply":"2024-11-26T09:26:38.187040Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom glob import glob\n\n\n# Define the image size to resize to (256x256 for pix2pix)\nIMG_HEIGHT = 512\nIMG_WIDTH = 512\n\n# Helper function to load an image\ndef load_image(image_file, is_target=False):\n    # Read the image file\n    image = tf.io.read_file(image_file)\n    \n    # Decode the image as PNG (adjust if using other formats like JPEG)\n    image = tf.image.decode_png(image, channels=3) #if is_target else tf.image.decode_png(image, channels=1)\n    \n    # Cast the image to float32 for processing\n    image = tf.cast(image, tf.float32)\n    \n    return image\n\n# Resize both input (skeleton) and target images to the required size (256x256)\ndef resize(input_image, target_image, height, width):\n    input_image = tf.image.resize(input_image, [height, width])\n    target_image = tf.image.resize(target_image, [height, width])\n    return input_image, target_image\n\n# Normalize both images to the range [-1, 1]\ndef normalize(input_image, target_image):\n    input_image = (input_image / 127.5) - 1\n    target_image = (target_image / 127.5) - 1\n    return input_image, target_image\n\n# Preprocessing function to load and preprocess both skeleton and target images\ndef load_train_image(skeleton_path, target_path):\n    input_image = load_image(skeleton_path, is_target=False)   # Grayscale (skeleton)\n    target_image = load_image(target_path, is_target=True)     # RGB (target)\n\n    # Resize both images to (256, 256)\n    input_image, target_image = resize(input_image, target_image, IMG_HEIGHT, IMG_WIDTH)\n    \n    # Normalize the images to the range [-1, 1]\n    input_image, target_image = normalize(input_image, target_image)\n    \n    return input_image, target_image\n\n# Function to load dataset as TensorFlow Dataset object\ndef load_dataset(dataset_path, batch_size):\n    skeleton_images = []\n    target_images = []\n\n    # Iterate over each train_X_img and train_X_label subfolder\n    for i in range(1, 11):  # Assuming the folder names are train_1_img to train_10_label\n        target_dir = os.path.join(dataset_path, f'train_{i}_img/')\n        skeleton_dir = os.path.join(dataset_path, f'train_{i}_label/')\n        \n        # Collect all image file paths from both skeleton and target directories\n        skeleton_images.extend(sorted(glob(os.path.join(skeleton_dir, '*.png'))))\n        target_images.extend(sorted(glob(os.path.join(target_dir, '*.png'))))\n    \n    # Ensure that skeletons and targets have the same number of files\n    assert len(skeleton_images) == len(target_images), \"Mismatch in the number of skeleton and target images\"\n\n    # Create a TensorFlow Dataset from the file paths\n    dataset = tf.data.Dataset.from_tensor_slices((skeleton_images, target_images))\n    \n    # Map the load_train_image function to each image pair\n    dataset = dataset.map(lambda skeleton_path, target_path: tf.py_function(\n        load_train_image, [skeleton_path, target_path], [tf.float32, tf.float32]))\n\n    # Shuffle, batch, and prefetch the dataset for performance\n    dataset = dataset.shuffle(buffer_size=400)  # Change this based on dataset size\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    \n    print(\"dataset created\")\n    \n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-11-26T09:26:40.878424Z","iopub.execute_input":"2024-11-26T09:26:40.879118Z","iopub.status.idle":"2024-11-26T09:26:40.889068Z","shell.execute_reply.started":"2024-11-26T09:26:40.879086Z","shell.execute_reply":"2024-11-26T09:26:40.888238Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\n# Residual Block\ndef residual_block(x, filters):\n    shortcut = x\n    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    return layers.Add()([shortcut, x])\n\n# Global Generator Network\ndef global_generator(input_shape=(256, 256, 3), filters=64, n_residual_blocks=9):\n    inputs = layers.Input(shape=input_shape)\n    x = layers.Conv2D(filters, kernel_size=7, strides=1, padding='same')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    # Downsampling\n    for _ in range(2):\n        filters *= 2\n        x = layers.Conv2D(filters, kernel_size=3, strides=2, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n\n    # Residual Blocks\n    for _ in range(n_residual_blocks):\n        x = residual_block(x, filters)\n\n    # Upsampling\n    for _ in range(2):\n        filters //= 2\n        x = layers.Conv2DTranspose(filters, kernel_size=3, strides=2, padding='same', output_padding=1)(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n\n    outputs = layers.Conv2D(3, kernel_size=7, strides=1, padding='same', activation='tanh')(x)\n    return Model(inputs, outputs, name=\"GlobalGenerator\")\n\n# Local Enhancement Network\ndef local_enhancement_network(input_shape=(512, 512, 3), filters=64, n_residual_blocks=3):\n    high_res_input = layers.Input(shape=input_shape)\n    low_res_input = layers.Input(shape=(256, 256, 3))  # Output from Global Generator\n\n    # Downsample high-resolution input\n    x = layers.Conv2D(filters, kernel_size=7, strides=1, padding='same')(high_res_input)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    for _ in range(2):\n        filters *= 2\n        x = layers.Conv2D(filters, kernel_size=3, strides=2, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n\n    # Resize low-resolution input to match the high-resolution downsampled size (128x128)\n    low_res_resized = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(low_res_input)\n    low_res_resized = layers.AveragePooling2D(pool_size=(2, 2))(low_res_resized)  # Downsample to 128x128\n    low_res_resized = layers.Conv2D(filters, kernel_size=1, strides=1, padding='same')(low_res_resized)  # Match depth\n\n    # **Fix: Adjust the low-res input to have the same channel depth as the high-res input**\n    low_res_resized = layers.Conv2D(filters , kernel_size=1, strides=1, padding='same')(low_res_resized)  # Match depth to high-res\n\n    # Concatenate downsampled high-res and resized low-res inputs\n    x = layers.Concatenate()([x, low_res_resized])\n\n    # Residual Blocks\n    for _ in range(n_residual_blocks):\n        x = residual_block(x, filters*2)\n\n    # Upsample\n    for _ in range(2):  # Two upsampling steps\n        filters //= 2\n        x = layers.Conv2DTranspose(filters, kernel_size=3, strides=2, padding='same', output_padding=1)(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.ReLU()(x)\n\n    # Output Layer\n    outputs = layers.Conv2D(3, kernel_size=7, strides=1, padding='same', activation='tanh')(x)\n    return Model([high_res_input, low_res_input], outputs, name=\"LocalEnhancementNetwork\")\n\n# Complete pix2pixHD Generator\ndef pix2pixHD_generator():\n    # Create Global Generator\n    global_gen = global_generator()\n\n    # Create Local Enhancement Network\n    local_enhancer = local_enhancement_network()\n\n    # Define high-resolution input\n    high_res_input = layers.Input(shape=(512, 512, 3))\n\n    # Downsample high-res input for Global Generator\n    low_res_input = layers.AveragePooling2D(pool_size=(2, 2))(high_res_input)\n\n    # Global Generator output\n    global_output = global_gen(low_res_input)\n\n    # Local Enhancement Network output\n    final_output = local_enhancer([high_res_input, global_output])\n\n    return Model(high_res_input, final_output, name=\"pix2pixHD_Generator\")\n\n# Create the generator model\ngenerator_model = pix2pixHD_generator()\n\n# Summary of the generator\ngenerator_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:33:14.221097Z","iopub.execute_input":"2024-11-26T09:33:14.221447Z","iopub.status.idle":"2024-11-26T09:33:14.732676Z","shell.execute_reply.started":"2024-11-26T09:33:14.221417Z","shell.execute_reply":"2024-11-26T09:33:14.731898Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"pix2pixHD_Generator\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pix2pixHD_Generator\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_46      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ average_pooling2d_3 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer_46[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)  │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ GlobalGenerator     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │ \u001b[38;5;34m11,399,171\u001b[0m │ average_pooling2… │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ LocalEnhancementNe… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │ \u001b[38;5;34m15,364,099\u001b[0m │ input_layer_46[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │ GlobalGenerator[\u001b[38;5;34m…\u001b[0m │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_46      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ average_pooling2d_3 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ GlobalGenerator     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">11,399,171</span> │ average_pooling2… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ LocalEnhancementNe… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15,364,099</span> │ input_layer_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │ GlobalGenerator[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,763,270\u001b[0m (102.09 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,763,270</span> (102.09 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,745,350\u001b[0m (102.03 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,745,350</span> (102.03 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,920\u001b[0m (70.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,920</span> (70.00 KB)\n</pre>\n"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"def downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(\n        layers.Conv2D(filters, size, strides=2, padding='same', \n                      kernel_initializer=initializer, use_bias=False))\n \n    if apply_batchnorm:\n        result.add(layers.BatchNormalization())\n \n    result.add(layers.LeakyReLU())\n    return result\n\ndef Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    input_img = layers.Input(shape=[512, 512, 3], name='input_image')\n    target_img = layers.Input(shape=[512, 512, 3], name='target_image')\n\n    x = layers.concatenate([input_img, target_img])  # (bs, 256, 256, 4)\n    \n    down1 = downsample(64, 4, False)(x)  # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1)  # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2)  # (bs, 32, 32, 256)\n    \n    zero_pad1 = layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n    \n    batchnorm1 = layers.BatchNormalization()(conv)\n    leaky_relu = layers.LeakyReLU()(batchnorm1)\n    \n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n    \n    last = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n    \n    return tf.keras.Model(inputs=[input_img, target_img], outputs=last)\n\ndis = Discriminator()\ndis.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-26T09:26:42.519827Z","iopub.execute_input":"2024-11-26T09:26:42.520602Z","iopub.status.idle":"2024-11-26T09:26:42.613233Z","shell.execute_reply.started":"2024-11-26T09:26:42.520568Z","shell.execute_reply":"2024-11-26T09:26:42.612388Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_image         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ target_image        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m6\u001b[0m)                │            │ target_image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │      \u001b[38;5;34m6,144\u001b[0m │ concatenate_5[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │    \u001b[38;5;34m131,584\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m525,312\u001b[0m │ sequential_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mSequential\u001b[0m)        │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ zero_padding2d      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m66\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ sequential_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mZeroPadding2D\u001b[0m)     │ \u001b[38;5;34m256\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_98 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m,    │  \u001b[38;5;34m2,097,152\u001b[0m │ zero_padding2d[\u001b[38;5;34m0\u001b[0m… │\n│                     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m,    │      \u001b[38;5;34m2,048\u001b[0m │ conv2d_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ leaky_re_lu_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ zero_padding2d_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m65\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_3[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mZeroPadding2D\u001b[0m)     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_99 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m1\u001b[0m) │      \u001b[38;5;34m8,193\u001b[0m │ zero_padding2d_1… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_image         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ target_image        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                │            │ target_image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │ concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ sequential_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ zero_padding2d      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,152</span> │ zero_padding2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv2d_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ leaky_re_lu_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ zero_padding2d_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,193</span> │ zero_padding2d_1… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,770,433\u001b[0m (10.57 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,770,433</span> (10.57 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,768,641\u001b[0m (10.56 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,768,641</span> (10.56 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"'''loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n    total_gen_loss = gan_loss + (100 * l1_loss)\n    return total_gen_loss, gan_loss, l1_loss\n\ndef discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss'''\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T15:00:03.699068Z","iopub.execute_input":"2024-11-24T15:00:03.699322Z","iopub.status.idle":"2024-11-24T15:00:03.704962Z","shell.execute_reply.started":"2024-11-24T15:00:03.699289Z","shell.execute_reply":"2024-11-24T15:00:03.704127Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\\n\\ndef generator_loss(disc_generated_output, gen_output, target):\\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\\n    total_gen_loss = gan_loss + (100 * l1_loss)\\n    return total_gen_loss, gan_loss, l1_loss\\n\\ndef discriminator_loss(disc_real_output, disc_generated_output):\\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\\n    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\\n    total_disc_loss = real_loss + generated_loss\\n    return total_disc_loss'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"@tf.function\ndef generator_loss(disc_generated_output, fake_images, real_images, feature_maps_real, feature_maps_fake, lambda_l1=10):\n    # Adversarial loss: Binary cross-entropy for fooling the discriminator\n    adversarial_loss = tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(disc_generated_output), logits=disc_generated_output)\n    )\n    \n    # Feature matching loss: L1 loss between feature maps from the real and fake images\n    feature_matching_loss = 0\n    #for real_feature, fake_feature in zip(feature_maps_real, feature_maps_fake):\n    feature_matching_loss = tf.reduce_mean(tf.abs(feature_maps_real - feature_maps_fake))\n    \n    # Optional L1 reconstruction loss: L1 loss between real and fake images\n    l1_loss = tf.reduce_mean(tf.abs(real_images - fake_images))\n    \n    # Combined loss\n    total_loss = adversarial_loss + feature_matching_loss + lambda_l1 * l1_loss\n    return total_loss\n@tf.function\ndef discriminator_loss(disc_real_output, disc_fake_output):\n    # Real image loss: Binary cross-entropy for classifying real images as real\n    real_loss = tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(disc_real_output), logits=disc_real_output)\n    )\n    \n    # Fake image loss: Binary cross-entropy for classifying fake images as fake\n    fake_loss = tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(disc_fake_output), logits=disc_fake_output)\n    )\n    \n    # Combined loss\n    total_loss = real_loss + fake_loss\n    return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:00:03.705991Z","iopub.execute_input":"2024-11-24T15:00:03.706251Z","iopub.status.idle":"2024-11-24T15:00:03.721967Z","shell.execute_reply.started":"2024-11-24T15:00:03.706225Z","shell.execute_reply":"2024-11-24T15:00:03.721262Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# Check if GPU is available and print the devices\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Enable dynamic memory growth\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"{len(gpus)} GPU(s) available: {[gpu.name for gpu in gpus]}\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"No GPU available. Running on CPU.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-24T15:00:03.722872Z","iopub.execute_input":"2024-11-24T15:00:03.723135Z","iopub.status.idle":"2024-11-24T15:00:03.737206Z","shell.execute_reply.started":"2024-11-24T15:00:03.723081Z","shell.execute_reply":"2024-11-24T15:00:03.736418Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Physical devices cannot be modified after being initialized\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nvgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\nvgg_model.trainable = False\n\n@tf.function\ndef train_step(input_image, target, generator, discriminator):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(input_image, training=True)\n        disc_real_output = discriminator([input_image, target], training=True)\n        disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n        y_true_features = vgg_model(target)\n        y_pred_features = vgg_model(gen_output)\n        gen_total_loss = generator_loss(disc_generated_output, gen_output, target, y_true_features, y_pred_features)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n        \n    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n    \n    return gen_total_loss, disc_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T15:00:03.738145Z","iopub.execute_input":"2024-11-24T15:00:03.738459Z","iopub.status.idle":"2024-11-24T15:00:04.326053Z","shell.execute_reply.started":"2024-11-24T15:00:03.738434Z","shell.execute_reply":"2024-11-24T15:00:04.325349Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def fit(generator, discriminator, dataset, epochs):\n    cardinality = tf.data.experimental.cardinality(dataset)\n    num_batches = cardinality.numpy()\n    for epoch in range(epochs):\n        start = time.time()\n        batch = 1\n        for input_image, target in dataset:\n            print(\"Epoch no.: \" + str(epoch) + \" Batch no.: \" + str(batch) + \"/\" + str(num_batches))\n            gen_loss, disc_loss = train_step(input_image, target, generator, discriminator)\n            batch+=1\n        print(f\"Epoch {epoch+1}/{epochs} | Generator Loss: {gen_loss} | Discriminator Loss: {disc_loss}\")\n        print(f'Time taken for epoch {epoch+1} is {time.time()-start} sec\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T15:00:04.327045Z","iopub.execute_input":"2024-11-24T15:00:04.327307Z","iopub.status.idle":"2024-11-24T15:00:04.332997Z","shell.execute_reply.started":"2024-11-24T15:00:04.327281Z","shell.execute_reply":"2024-11-24T15:00:04.332097Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"BATCH_SIZE = 8  # Adjust based on your GPU memory\n\n# Load dataset\nDATASET_PATH = \"/kaggle/input/frame-pose-dataset/video-pose dataset/data-meta\"\ntrain_dataset = load_dataset(DATASET_PATH, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T15:00:04.335100Z","iopub.execute_input":"2024-11-24T15:00:04.335359Z","iopub.status.idle":"2024-11-24T15:00:05.558768Z","shell.execute_reply.started":"2024-11-24T15:00:04.335327Z","shell.execute_reply":"2024-11-24T15:00:05.557900Z"},"trusted":true},"outputs":[{"name":"stdout","text":"dataset created\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Create generator and discriminator\ngenerator = global_generator()\ndiscriminator = Discriminator()\n\n# Assume dataset is already loaded and preprocessed\nfit(generator, discriminator, train_dataset, 10)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T15:00:05.559889Z","iopub.execute_input":"2024-11-24T15:00:05.560186Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"Epoch no.: 0 Batch no.: 1/3487\nEpoch no.: 0 Batch no.: 2/3487\nEpoch no.: 0 Batch no.: 3/3487\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assume dataset is already loaded and preprocessed\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(generator, discriminator, dataset, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch no.: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Batch no.: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(num_batches))\n\u001b[1;32m      9\u001b[0m     gen_loss, disc_loss \u001b[38;5;241m=\u001b[39m train_step(input_image, target, generator, discriminator)\n\u001b[0;32m---> 10\u001b[0m     batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Generator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Discriminator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisc_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime taken for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":null},{"cell_type":"code","source":"generator.save('/kaggle/working/generator-pretrained-pix2pix.h5')\ndiscriminator.save('/kaggle/working/discriminator-pretrained-pix2pix.h5')","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:56:21.758564Z","iopub.execute_input":"2024-11-24T14:56:21.759216Z","iopub.status.idle":"2024-11-24T14:56:22.020073Z","shell.execute_reply.started":"2024-11-24T14:56:21.759184Z","shell.execute_reply":"2024-11-24T14:56:22.019122Z"},"trusted":true},"outputs":[],"execution_count":93},{"cell_type":"code","source":"generator = tf.keras.models.load_model('/kaggle/working/generator-pretrained-pix2pix.h5')\ndiscriminator = tf.keras.models.load_model('/kaggle/working/discriminator-pretrained-pix2pix.h5')","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:56:54.523837Z","iopub.execute_input":"2024-11-24T14:56:54.524226Z","iopub.status.idle":"2024-11-24T14:56:55.022640Z","shell.execute_reply.started":"2024-11-24T14:56:54.524196Z","shell.execute_reply":"2024-11-24T14:56:55.021799Z"},"trusted":true},"outputs":[],"execution_count":95}]}